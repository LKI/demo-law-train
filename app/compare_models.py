# æ³•å¾‹QAæ¨¡å‹å¯¹æ¯”æµ‹è¯• - æœ¬åœ°æ•°æ®/æƒé‡ç‰ˆ
from pathlib import Path
import json
import random
import re

import torch
from peft import PeftModel
from transformers import AutoModelForCausalLM, AutoTokenizer

print("=" * 70)
print("æ³•å¾‹QAæ¨¡å‹å¯¹æ¯”æµ‹è¯• - æœ¬åœ°æ•°æ®/æƒé‡ç‰ˆ")
print("=" * 70)

# ==================== é…ç½® ====================
BASE_DIR = Path(__file__).resolve().parent
BASE_MODEL = BASE_DIR / "models" / "base"
LORA_MODEL = BASE_DIR / "models" / "law-qa-qwen-lora"
DATA_FILE = BASE_DIR / "data" / "test-data.jsonl"
NUM_SAMPLES = 5  # æµ‹è¯•æ ·æœ¬æ•°é‡


def load_jsonl(path, limit=None):
    """Load line-delimited JSON, optionally sampling a subset."""
    records = []
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            records.append(json.loads(line))
    if limit is not None and len(records) > limit:
        random.seed(42)
        records = random.sample(records, limit)
    return records


# ==================== åŠ è½½æµ‹è¯•æ•°æ® ====================
print("\nğŸ“– åŠ è½½æµ‹è¯•æ•°æ®...")
data = load_jsonl(DATA_FILE)
print(f"âœ… æ•°æ®é›†å¤§å°: {len(data):,} æ¡")

# æå–é—®é¢˜å’Œå‚è€ƒç­”æ¡ˆï¼ˆé€‚é… input/output å­—æ®µï¼‰
test_cases = []
for sample in data:
    question = sample.get("input")
    reference = sample.get("output")
    if question and reference:
        test_cases.append(
            {
                "question": question,
                "reference": reference,
                "source": sample.get("id", "unknown"),
            }
        )

# éšæœºæŠ½å–æ ·æœ¬
random.seed(42)
test_cases = random.sample(test_cases, min(NUM_SAMPLES, len(test_cases)))

print(f"âœ… æœ‰æ•ˆæµ‹è¯•ç”¨ä¾‹: {len(test_cases)} ä¸ª\n")

if not test_cases:
    raise SystemExit("æ²¡æœ‰å¯ç”¨çš„æµ‹è¯•ç”¨ä¾‹ï¼Œè¯·æ£€æŸ¥æ•°æ®æ–‡ä»¶ã€‚")

# ==================== åŠ è½½æ¨¡å‹ ====================
print("â³ åŠ è½½æ¨¡å‹...")
tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)

base = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL, torch_dtype=torch.float16, device_map="auto", trust_remote_code=True
)

finetuned_base = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL, torch_dtype=torch.float16, device_map="auto", trust_remote_code=True
)

finetuned = PeftModel.from_pretrained(finetuned_base, LORA_MODEL)

print("âœ… åŠ è½½å®Œæˆ\n")


# ==================== ç›¸ä¼¼åº¦è®¡ç®—å‡½æ•° ====================
def calculate_similarity(generated, reference):
    """è®¡ç®—ç”Ÿæˆç­”æ¡ˆä¸å‚è€ƒç­”æ¡ˆçš„ç›¸ä¼¼åº¦"""

    # æå–ä¸­æ–‡è¯æ±‡ï¼ˆ2å­—åŠä»¥ä¸Šï¼‰
    gen_words = set(re.findall(r"[\u4e00-\u9fff]{2,}", generated))
    ref_words = set(re.findall(r"[\u4e00-\u9fff]{2,}", reference))

    # è¯æ±‡é‡å ç‡
    if len(ref_words) > 0:
        common = gen_words & ref_words
        word_overlap = len(common) / len(ref_words)
        common_count = len(common)
    else:
        word_overlap = 0
        common_count = 0

    # å…³é”®çŸ­è¯­è¦†ç›–ï¼ˆ4å­—åŠä»¥ä¸Šï¼‰
    ref_phrases = set(re.findall(r"[\u4e00-\u9fff]{4,}", reference))
    if len(ref_phrases) > 0:
        phrase_hits = sum(1 for phrase in ref_phrases if phrase in generated)
        phrase_coverage = phrase_hits / len(ref_phrases)
    else:
        phrase_coverage = 0

    # ç»¼åˆå¾—åˆ†
    score = (word_overlap * 0.6 + phrase_coverage * 0.4) * 100

    return {
        "score": score,
        "word_overlap": word_overlap,
        "phrase_coverage": phrase_coverage,
        "common_words": common_count,
        "total_ref_words": len(ref_words),
    }


# ==================== å¯¹æ¯”æµ‹è¯• ====================
results = []

for i, test in enumerate(test_cases, 1):
    print(f"{'=' * 70}")
    print(f"æµ‹è¯• {i}/{len(test_cases)}")
    print(f"{'=' * 70}")
    print(f"æ¥æº: {test['source']}")

    print("\nã€é—®é¢˜ã€‘")
    print(test["question"])

    print("\nã€å‚è€ƒç­”æ¡ˆã€‘ï¼ˆå‰200å­—ï¼‰")
    ref_preview = (
        test["reference"][:200] + "..."
        if len(test["reference"]) > 200
        else test["reference"]
    )
    print(ref_preview)

    # å‡†å¤‡è¾“å…¥
    messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ³•å¾‹å’¨è¯¢åŠ©æ‰‹ã€‚"},
        {"role": "user", "content": test["question"]},
    ]
    text = tokenizer.apply_chat_template(
        messages, tokenize=False, add_generation_prompt=True
    )
    inputs = tokenizer(text, return_tensors="pt").to(base.device)

    # åŸºåº§æ¨¡å‹ç”Ÿæˆ
    print("\nã€åŸºåº§æ¨¡å‹å›ç­”ã€‘")
    print("-" * 70)
    with torch.no_grad():
        out = base.generate(**inputs, max_new_tokens=200, temperature=0.7)
    base_response = tokenizer.decode(
        out[0][inputs["input_ids"].shape[1] :], skip_special_tokens=True
    )
    print(base_response)

    # è®¡ç®—åŸºåº§æ¨¡å‹ç›¸ä¼¼åº¦
    base_sim = calculate_similarity(base_response, test["reference"])
    print("\nğŸ“Š ä¸å‚è€ƒç­”æ¡ˆçš„ç›¸ä¼¼åº¦:")
    print(f"  â€¢ ç»¼åˆå¾—åˆ†: {base_sim['score']:.1f}/100")
    print(
        f"  â€¢ è¯æ±‡é‡å : {base_sim['word_overlap'] * 100:.1f}% ({base_sim['common_words']}/{base_sim['total_ref_words']})"
    )
    print(f"  â€¢ çŸ­è¯­è¦†ç›–: {base_sim['phrase_coverage'] * 100:.1f}%")

    # å¾®è°ƒæ¨¡å‹ç”Ÿæˆ
    print("\nã€å¾®è°ƒæ¨¡å‹å›ç­”ã€‘")
    print("-" * 70)
    with torch.no_grad():
        out = finetuned.generate(**inputs, max_new_tokens=200, temperature=0.7)
    ft_response = tokenizer.decode(
        out[0][inputs["input_ids"].shape[1] :], skip_special_tokens=True
    )
    print(ft_response)

    # è®¡ç®—å¾®è°ƒæ¨¡å‹ç›¸ä¼¼åº¦
    ft_sim = calculate_similarity(ft_response, test["reference"])
    print("\nğŸ“Š ä¸å‚è€ƒç­”æ¡ˆçš„ç›¸ä¼¼åº¦:")
    print(f"  â€¢ ç»¼åˆå¾—åˆ†: {ft_sim['score']:.1f}/100")
    print(
        f"  â€¢ è¯æ±‡é‡å : {ft_sim['word_overlap'] * 100:.1f}% ({ft_sim['common_words']}/{ft_sim['total_ref_words']})"
    )
    print(f"  â€¢ çŸ­è¯­è¦†ç›–: {ft_sim['phrase_coverage'] * 100:.1f}%")

    # å¯¹æ¯”ç»“æœ
    improvement = ft_sim["score"] - base_sim["score"]

    print(f"\n{'ğŸ¯ å¯¹æ¯”ç»“æœ':=^70}")
    if improvement > 15:
        verdict = f"ğŸ† å¾®è°ƒæ¨¡å‹æ˜¾è‘—æ›´å¥½ï¼æå‡ {improvement:.1f} åˆ†"
    elif improvement > 10:
        verdict = f"âœ… å¾®è°ƒæ¨¡å‹æ˜æ˜¾æ›´å¥½ï¼Œæå‡ {improvement:.1f} åˆ†"
    elif improvement > 5:
        verdict = f"ğŸ‘ å¾®è°ƒæ¨¡å‹æ›´å¥½ï¼Œæå‡ {improvement:.1f} åˆ†"
    elif improvement > 0:
        verdict = f"âœ… å¾®è°ƒæ¨¡å‹ç•¥å¥½ï¼Œæå‡ {improvement:.1f} åˆ†"
    elif improvement > -5:
        verdict = f"ğŸ¤ ä¸¤è€…æ¥è¿‘ï¼Œå·®è· {abs(improvement):.1f} åˆ†"
    else:
        verdict = f"âš ï¸ åŸºåº§æ¨¡å‹æ›´å¥½ï¼Œå·®è· {abs(improvement):.1f} åˆ†"

    print(verdict)
    print("=" * 70)
    print()

    results.append(
        {
            "base_score": base_sim["score"],
            "ft_score": ft_sim["score"],
            "improvement": improvement,
        }
    )

# ==================== ç»¼åˆè¯„ä¼°æŠ¥å‘Š ====================
print("\n")
print("=" * 70)
print("ğŸ“Š ç»¼åˆè¯„ä¼°æŠ¥å‘Š")
print("=" * 70)

avg_base = sum(r["base_score"] for r in results) / len(results)
avg_ft = sum(r["ft_score"] for r in results) / len(results)
avg_improvement = sum(r["improvement"] for r in results) / len(results)

print("\nã€å¹³å‡ç›¸ä¼¼åº¦å¾—åˆ†ã€‘")
print(f"  åŸºåº§æ¨¡å‹: {avg_base:.1f}/100")
print(f"  å¾®è°ƒæ¨¡å‹: {avg_ft:.1f}/100")
print(
    f"  å¹³å‡æå‡: {avg_improvement:+.1f} åˆ† ({(avg_improvement / avg_base) * 100:+.1f}%)"
)

# èƒœè´Ÿç»Ÿè®¡
wins = sum(1 for r in results if r["improvement"] > 5)
draws = sum(1 for r in results if -5 <= r["improvement"] <= 5)
losses = sum(1 for r in results if r["improvement"] < -5)

print("\nã€å¯¹æˆ˜æˆç»©ã€‘")
print(f"  å¾®è°ƒæ˜æ˜¾æ›´å¥½: {wins}/{len(results)} ({wins / len(results) * 100:.0f}%)")
print(f"  ä¸¤è€…æ¥è¿‘: {draws}/{len(results)} ({draws / len(results) * 100:.0f}%)")
print(f"  åŸºåº§æ›´å¥½: {losses}/{len(results)} ({losses / len(results) * 100:.0f}%)")

# ç»“è®º
print(f"\n{'ğŸ“ æœ€ç»ˆç»“è®º':=^70}")

if avg_improvement > 15:
    grade = "A+ (ä¼˜ç§€)"
    conclusion = "âœ… å¾®è°ƒæ•ˆæœæ˜¾è‘—ï¼æ¨¡å‹åœ¨è®­ç»ƒæ•°æ®ä¸Šçš„è¡¨ç°è¿œè¶…åŸºåº§æ¨¡å‹ã€‚"
elif avg_improvement > 10:
    grade = "A (è‰¯å¥½)"
    conclusion = "âœ… å¾®è°ƒæ•ˆæœæ˜æ˜¾ï¼Œæ¨¡å‹æ˜æ˜¾ä¼˜äºåŸºåº§æ¨¡å‹ã€‚"
elif avg_improvement > 5:
    grade = "B+ (åˆæ ¼)"
    conclusion = "âœ… å¾®è°ƒæœ‰æ•ˆï¼Œæ¨¡å‹ä¼˜äºåŸºåº§æ¨¡å‹ã€‚"
elif avg_improvement > 0:
    grade = "B (ä¸€èˆ¬)"
    conclusion = "âš ï¸ å¾®è°ƒæ•ˆæœæœ‰é™ï¼Œæå‡ä¸å¤Ÿæ˜æ˜¾ã€‚"
else:
    grade = "C (éœ€æ”¹è¿›)"
    conclusion = "âš ï¸ å¾®è°ƒæ•ˆæœä¸æ˜æ˜¾ï¼Œéœ€è¦æ£€æŸ¥è®­ç»ƒè¿‡ç¨‹ã€‚"

print(f"\n{conclusion}")
print(f"\nå¾®è°ƒæ•ˆæœè¯„çº§: {grade}")

if avg_improvement < 10:
    print("\nğŸ’¡ æ”¹è¿›å»ºè®®:")
    print("  â€¢ å¢åŠ è®­ç»ƒè½®æ•°ï¼ˆ1 â†’ 2-3 Epochsï¼‰")
    print("  â€¢ å¢å¤§ LoRA rankï¼ˆr=4 â†’ r=8ï¼‰")
    print("  â€¢ è°ƒæ•´å­¦ä¹ ç‡")
    print("  â€¢ æ£€æŸ¥æ•°æ®è´¨é‡")

print("\n" + "=" * 70)
print("âœ… æµ‹è¯•å®Œæˆ")
print("=" * 70)
